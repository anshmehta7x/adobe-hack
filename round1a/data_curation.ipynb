{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "714b8e0f",
   "metadata": {},
   "source": [
    "# PDF Text Extraction and Analysis - Batch Processing for Sample-1a\n",
    "\n",
    "## Setting Input and Output Directories\n",
    "This notebook processes multiple PDF documents from the sample-1a dataset to extract text and structure. The next cell sets up the input PDF directory path and the corresponding JSON output directory to build a larger dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f34e69a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 7 PDF files in the directory.\n",
      "Found 5 JSON files in the directory.\n",
      "Processing 5 PDFs with matching JSON files.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "# Define the input and output directories\n",
    "pdf_dir = \"../hackathon-task/sample-1a/Datasets/Pdfs/\"\n",
    "json_dir = \"../hackathon-task/sample-1a/Datasets/Output.json/\"\n",
    "\n",
    "# Get list of all PDF files in the directory\n",
    "pdf_files = glob.glob(os.path.join(pdf_dir, \"*.pdf\"))\n",
    "\n",
    "# Get list of all JSON files in the directory\n",
    "json_files = glob.glob(os.path.join(json_dir, \"*.json\"))\n",
    "\n",
    "# Map PDFs to their corresponding JSON files (if available)\n",
    "pdf_to_json = {}\n",
    "for pdf_path in pdf_files:\n",
    "    pdf_basename = os.path.basename(pdf_path).split('.')[0]\n",
    "    json_path = os.path.join(json_dir, f\"{pdf_basename}.json\")\n",
    "    \n",
    "    # Only include PDFs that have corresponding JSON files\n",
    "    if os.path.exists(json_path):\n",
    "        pdf_to_json[pdf_path] = json_path\n",
    "\n",
    "print(f\"Found {len(pdf_files)} PDF files in the directory.\")\n",
    "print(f\"Found {len(json_files)} JSON files in the directory.\")\n",
    "print(f\"Processing {len(pdf_to_json)} PDFs with matching JSON files.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e61dee08",
   "metadata": {},
   "source": [
    "# Text Extraction Using MultiProcessing\n",
    "The code below imports the custom `extract` module to extract text from the PDF file. It uses multiprocessing to efficiently extract text snippets from all pages and retrieves the page dimensions which will be needed for feature engineering later."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2956ce8",
   "metadata": {},
   "source": [
    "# Batch Processing Function\n",
    "Let's create a function that processes a single PDF file and its corresponding JSON output file from the sample-1a dataset. This function will encapsulate all the processing steps from the original notebook so we can easily apply them to multiple files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1621ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import extract\n",
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "from collections import Counter\n",
    "from thefuzz import fuzz\n",
    "import os\n",
    "\n",
    "def process_pdf_file(pdf_path, json_path):\n",
    "    \"\"\"Process a single PDF file and its corresponding JSON file\n",
    "    \n",
    "    Args:\n",
    "        pdf_path: Path to the PDF file\n",
    "        json_path: Path to the corresponding JSON file with ground truth\n",
    "        output_csv_dir: Directory to save the output CSV\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame containing the labeled text lines\n",
    "    \"\"\"\n",
    "    print(f\"Processing: {os.path.basename(pdf_path)}\")\n",
    "    \n",
    "    # Initialize the extractor\n",
    "    extractor = extract.TextExtractor(pdf_path)\n",
    "    texts = extractor.extract_text_from_all_pages_multiprocessing()\n",
    "    \n",
    "    # Get page dimensions\n",
    "    dims = extractor.get_page_dimensions(0)\n",
    "    PAGE_WIDTH = dims[\"width\"]\n",
    "    PAGE_HEIGHT = dims[\"height\"]\n",
    "    \n",
    "    # Sort text snippets\n",
    "    sorted_snippets = sorted(texts, key=lambda s: (s['page'], s['y_position'], s['bbox'][0]))\n",
    "    \n",
    "    # Group snippets into lines and process them\n",
    "    def group_snippets_into_lines(snippets, y_tolerance=2.0):\n",
    "        if not snippets:\n",
    "            return []\n",
    "            \n",
    "        lines = []\n",
    "        current_line_snippets = [snippets[0]]\n",
    "        \n",
    "        for i in range(1, len(snippets)):\n",
    "            prev_snippet = snippets[i-1]\n",
    "            current_snippet = snippets[i]\n",
    "            \n",
    "            if (current_snippet['page'] == prev_snippet['page'] and \n",
    "                abs(current_snippet['y_position'] - prev_snippet['y_position']) < y_tolerance):\n",
    "                current_line_snippets.append(current_snippet)\n",
    "            else:\n",
    "                lines.append(current_line_snippets)\n",
    "                current_line_snippets = [current_snippet]\n",
    "                \n",
    "        lines.append(current_line_snippets)\n",
    "        return lines\n",
    "\n",
    "    def process_lines(grouped_lines):\n",
    "        processed_lines = []\n",
    "        for line_snippets in grouped_lines:\n",
    "            line_snippets.sort(key=lambda s: s['bbox'][0])\n",
    "            \n",
    "            full_text = \"\".join(s['text'] for s in line_snippets).strip()\n",
    "            if not full_text:\n",
    "                continue\n",
    "                \n",
    "            import re\n",
    "            \n",
    "            toc_pattern = re.compile(r'^(.*?)\\.{3,}\\s*\\d+$')\n",
    "            match = toc_pattern.match(full_text)\n",
    "            if match:\n",
    "                full_text = match.group(1).strip()\n",
    "            \n",
    "            elif full_text.endswith('...') and full_text.count('.') > 3:\n",
    "                full_text = full_text.rstrip('.')\n",
    "\n",
    "            x0 = min(s['bbox'][0] for s in line_snippets)\n",
    "            y0 = min(s['bbox'][1] for s in line_snippets)\n",
    "            x1 = max(s['bbox'][2] for s in line_snippets)\n",
    "            y1 = max(s['bbox'][3] for s in line_snippets)\n",
    "            \n",
    "            avg_font_size = sum(s['font_size'] for s in line_snippets) / len(line_snippets)\n",
    "            \n",
    "            processed_lines.append({\n",
    "                \"text\": full_text,\n",
    "                \"page\": line_snippets[0]['page'],\n",
    "                \"avg_font_size\": avg_font_size,\n",
    "                \"y_position\": line_snippets[0]['y_position'],\n",
    "                \"bbox\": (x0, y0, x1, y1),\n",
    "                \"font_name\": line_snippets[0]['font_name'],\n",
    "                \"source_pdf\": os.path.basename(pdf_path)\n",
    "            })\n",
    "        return processed_lines\n",
    "\n",
    "    grouped_lines = group_snippets_into_lines(sorted_snippets)\n",
    "    final_lines = process_lines(grouped_lines)\n",
    "    \n",
    "    # Get document statistics\n",
    "    def get_doc_stats(lines):\n",
    "        font_sizes = [round(l['avg_font_size'], 2) for l in lines if l['text']]\n",
    "        if not font_sizes:\n",
    "            return {'modal_font_size': 10.0}\n",
    "            \n",
    "        modal_font_size = Counter(font_sizes).most_common(1)[0][0]\n",
    "        return {\n",
    "            'modal_font_size': modal_font_size\n",
    "        }\n",
    "    \n",
    "    document_stats = get_doc_stats(final_lines)\n",
    "    \n",
    "    # Engineer features\n",
    "    def engineer_features(lines, doc_stats):\n",
    "        modal_font_size = doc_stats['modal_font_size']\n",
    "        \n",
    "        for i, line in enumerate(lines):\n",
    "            font_name_lower = line['font_name'].lower()\n",
    "            line['is_bold'] = any(indicator in font_name_lower for indicator in ['bold', 'black', 'heavy', 'sembold'])\n",
    "\n",
    "            line['is_all_caps'] = line['text'].isupper() and len(line['text']) > 3\n",
    "            \n",
    "            line['text_len'] = len(line['text'])\n",
    "\n",
    "            numbering_pattern = re.compile(\n",
    "                r'^\\s*(?:(?:Chapter|Section)\\s+[\\w\\d]+|'\n",
    "                r'\\d{1,2}(?:\\.\\d{1,2})*\\.?|'\n",
    "                r'[A-Z]\\.|'\n",
    "                r'\\([a-z]\\)|'\n",
    "                r'[ivx]+\\.)'\n",
    "            )\n",
    "            line['starts_with_numbering'] = bool(numbering_pattern.match(line['text']))\n",
    "\n",
    "            if modal_font_size > 0:\n",
    "                line['relative_font_size'] = line['avg_font_size'] / modal_font_size\n",
    "            else:\n",
    "                line['relative_font_size'] = 1.0\n",
    "\n",
    "            line['norm_y_pos'] = line['y_position'] / PAGE_HEIGHT\n",
    "\n",
    "            line_center = (line['bbox'][0] + line['bbox'][2]) / 2\n",
    "            page_center = PAGE_WIDTH / 2\n",
    "            line['is_centered'] = abs(line_center - page_center) < (0.1 * PAGE_WIDTH)\n",
    "\n",
    "            space_before = -1\n",
    "            space_after = -1\n",
    "            \n",
    "            if i > 0 and lines[i-1]['page'] == line['page']:\n",
    "                prev_line_bottom = lines[i-1]['bbox'][3]\n",
    "                current_line_top = line['bbox'][1]\n",
    "                space_before = current_line_top - prev_line_bottom\n",
    "                \n",
    "            if i < len(lines) - 1 and lines[i+1]['page'] == line['page']:\n",
    "                current_line_bottom = line['bbox'][3]\n",
    "                next_line_top = lines[i+1]['bbox'][1]\n",
    "                space_after = next_line_top - current_line_bottom\n",
    "\n",
    "            line['space_before'] = space_before\n",
    "            line['space_after'] = space_after\n",
    "            \n",
    "        return lines\n",
    "    \n",
    "    featured_lines = engineer_features(final_lines, document_stats)\n",
    "    \n",
    "    # Label dataset\n",
    "    def normalize_text(text):\n",
    "        if not isinstance(text, str):\n",
    "            return \"\"\n",
    "        text = text.lower()\n",
    "        text = text.strip()\n",
    "        text = re.sub(r'[\\u2013\\u2014]', '-', text)\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        return text\n",
    "\n",
    "    def create_labeled_dataset_final(featured_lines, ground_truth_headings, ground_truth_title, fuzz_threshold=85):\n",
    "        gt_headings_by_page = {}\n",
    "        for h in ground_truth_headings:\n",
    "            page = h['page']\n",
    "            if page not in gt_headings_by_page:\n",
    "                gt_headings_by_page[page] = []\n",
    "            gt_headings_by_page[page].append({\n",
    "                'level': h['level'],\n",
    "                'norm_text': normalize_text(h['text'])\n",
    "            })\n",
    "        \n",
    "        page_1_lines = [line for line in featured_lines if line['page'] == 1]\n",
    "        if page_1_lines:\n",
    "            title_candidate = max(page_1_lines, key=lambda l: l['relative_font_size'])\n",
    "            \n",
    "            norm_gt_title = normalize_text(ground_truth_title)\n",
    "            \n",
    "            title_text_to_check = normalize_text(title_candidate['text'])\n",
    "            candidate_index = featured_lines.index(title_candidate)\n",
    "            if candidate_index + 1 < len(featured_lines):\n",
    "                next_line = featured_lines[candidate_index+1]\n",
    "                if abs(next_line['avg_font_size'] - title_candidate['avg_font_size']) < 1:\n",
    "                     title_text_to_check += \" \" + normalize_text(next_line['text'])\n",
    "\n",
    "            if fuzz.partial_ratio(title_text_to_check, norm_gt_title) > fuzz_threshold:\n",
    "                title_candidate['label'] = 'Title'\n",
    "                if \" \" in title_text_to_check:\n",
    "                     featured_lines[candidate_index+1]['label'] = 'Title'\n",
    "                \n",
    "        for line in featured_lines:\n",
    "            if 'label' in line:\n",
    "                continue\n",
    "                \n",
    "            line['label'] = 'Body Text'\n",
    "            page_num = line['page']\n",
    "            \n",
    "            if page_num in gt_headings_by_page:\n",
    "                norm_line_text = normalize_text(line['text'])\n",
    "                \n",
    "                best_match = max(\n",
    "                    gt_headings_by_page[page_num],\n",
    "                    key=lambda h: fuzz.ratio(norm_line_text, h['norm_text']),\n",
    "                    default=None\n",
    "                )\n",
    "                \n",
    "                if best_match:\n",
    "                    score = fuzz.ratio(norm_line_text, best_match['norm_text'])\n",
    "                    if score > fuzz_threshold:\n",
    "                        line['label'] = best_match['level']\n",
    "                        \n",
    "        return featured_lines\n",
    "    \n",
    "    # Load ground truth data\n",
    "    with open(json_path, 'r', encoding='utf-8') as f:\n",
    "        out = json.load(f)\n",
    "    \n",
    "    # Correct page numbering\n",
    "    corrected_outline = [{**h, 'page': h['page'] + 1} for h in out['outline']]\n",
    "    \n",
    "    # Create labeled dataset\n",
    "    labeled_lines = create_labeled_dataset_final(featured_lines, corrected_outline, out['title'])\n",
    "    \n",
    "    # Fix page numbers to be 0-indexed\n",
    "    for line in labeled_lines:\n",
    "        line['page'] -= 1\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    df = pd.DataFrame(labeled_lines)\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23228f03",
   "metadata": {},
   "source": [
    "# Executing Batch Processing\n",
    "Now that we've created our function to process individual PDF files, let's create an output directory for our labeled data and then process all the PDF files with matching JSON files in batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9ca6f8cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing ../hackathon-task/sample-1a/Datasets/Pdfs/E0CCG5S239.pdf: process_pdf_file() missing 1 required positional argument: 'output_csv_dir'\n",
      "Error processing ../hackathon-task/sample-1a/Datasets/Pdfs/E0H1CM114.pdf: process_pdf_file() missing 1 required positional argument: 'output_csv_dir'\n",
      "Error processing ../hackathon-task/sample-1a/Datasets/Pdfs/E0CCG5S312.pdf: process_pdf_file() missing 1 required positional argument: 'output_csv_dir'\n",
      "Error processing ../hackathon-task/sample-1a/Datasets/Pdfs/STEMPathwaysFlyer.pdf: process_pdf_file() missing 1 required positional argument: 'output_csv_dir'\n",
      "Error processing ../hackathon-task/sample-1a/Datasets/Pdfs/TOPJUMP-PARTY-INVITATION-20161003-V01.pdf: process_pdf_file() missing 1 required positional argument: 'output_csv_dir'\n",
      "Successfully processed 0 PDF files.\n"
     ]
    }
   ],
   "source": [
    "# Create output directory for labeled data CSVs\n",
    "\n",
    "# Process all PDF files with matching JSON files\n",
    "all_dataframes = []\n",
    "\n",
    "for pdf_path, json_path in pdf_to_json.items():\n",
    "    try:\n",
    "        df = process_pdf_file(pdf_path, json_path)\n",
    "        all_dataframes.append(df)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {pdf_path}: {str(e)}\")\n",
    "\n",
    "print(f\"Successfully processed {len(all_dataframes)} PDF files.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "937941ab",
   "metadata": {},
   "source": [
    "# Combining All Data into a Single Dataset\n",
    "Let's combine all the individual labeled datasets into one large dataset, which will be useful for training machine learning models. We'll concatenate all the DataFrames and save the result to a single CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "539a2c1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No data to combine. Please check that PDF processing was successful.\n"
     ]
    }
   ],
   "source": [
    "# Combine all DataFrames into a single dataset\n",
    "if all_dataframes:\n",
    "    combined_df = pd.concat(all_dataframes, ignore_index=True)\n",
    "    \n",
    "    print(f\"Combined dataset created with {len(combined_df)} rows\")\n",
    "    \n",
    "    # Display statistics about the combined dataset\n",
    "    label_counts = combined_df['label'].value_counts()\n",
    "    print(\"\\nLabel distribution in the combined dataset:\")\n",
    "    print(label_counts)\n",
    "    \n",
    "    pdf_counts = combined_df['source_pdf'].value_counts()\n",
    "    print(\"\\nNumber of rows from each PDF:\")\n",
    "    print(pdf_counts)\n",
    "else:\n",
    "    print(\"No data to combine. Please check that PDF processing was successful.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc78806",
   "metadata": {},
   "source": [
    "# Finalizing the Dataset\n",
    "Let's finalize our dataset by renaming the combined dataset for clarity and generating additional statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5ac775c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No data to combine.\n"
     ]
    }
   ],
   "source": [
    "# We'll use the dataframes we've already created\n",
    "if all_dataframes:\n",
    "    # This is already our final DataFrame\n",
    "    final_df = combined_df\n",
    "    \n",
    "    # Save to CSV with a different name to distinguish it\n",
    "    final_csv_path = os.path.join(\"final_dataset.csv\")\n",
    "    final_df.to_csv(final_csv_path, index=False)\n",
    "    \n",
    "    print(f\"Created final dataset with {len(final_df)} rows from {len(all_dataframes)} PDFs\")\n",
    "    print(f\"Saved to {final_csv_path}\")\n",
    "    \n",
    "    # Display dataset statistics\n",
    "    print(\"\\nFinal Dataset Statistics:\")\n",
    "    print(f\"Total rows: {len(final_df)}\")\n",
    "    print(f\"Unique PDFs: {final_df['source_pdf'].nunique()}\")\n",
    "    print(\"\\nLabel distribution:\")\n",
    "    print(final_df['label'].value_counts())\n",
    "    \n",
    "    # Create a summary table by PDF source\n",
    "    pdf_summary = final_df.groupby('source_pdf')['label'].value_counts().unstack().fillna(0)\n",
    "    print(\"\\nRows by PDF and label type:\")\n",
    "    print(pdf_summary)\n",
    "else:\n",
    "    print(\"No data to combine.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
